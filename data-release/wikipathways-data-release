#!/usr/bin/env bash

###
# Run this script monthly in order to prepare and organize a regularly released set of curated content
#
# The script calls upon existing urls to trigger the retrieval or regeneration of an archive set,
#  e.g., https://wikipathways.org//wpi/batchDownload.php?species=Homo%20sapiens&fileType=gpml&tag=Curation:AnalysisCollection
#
# The retrieved files are equivalent to the zipped files found in /wpi/cache, 
#  e.g., "wikipathways_Homo_sapiens_Curation-AnalysisCollection__gpml.zip" for 
#  each of the species and file types specified below. 
#
# These archived sets are intended to serve as immutable data sources for downstream analyses.
#
# The script also stashes a copy of the lucene index into an index subdirectory 
###

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)" || exit 1

# Based on http://linuxcommand.org/lc3_wss0140.php
# and https://codeinthehole.com/tips/bash-error-reporting/
PROGNAME=$(basename "$0")

DATA="$1"
if [ ! -d "$DATA" ]; then
  echo "$PROGNAME: error: $DATA is not a directory" >&2
  exit 1
fi
DATE=`date +%Y%m%d`
URL_BASE='https://wikipathways.org/wpi/batchDownload.php' 

## setup dir
mkdir "$DATA"/"$DATE"
if ! cp "$SCRIPT_DIR"/index.php "$DATA"/"$DATE"/.index.php; then
        echo "ERROR: Can't copy index.php to new date dir" >&2
fi

# tag to collect
TAG='Curation:AnalysisCollection'

# file types to collect
declare -a TYPES=(
'gpml'
'svg'
)

## setup dirs
for h in "${TYPES[@]}"
do
        DIR="$DATA"/"$DATE"/"$h"
	mkdir "$DIR"
        if ! cp "$SCRIPT_DIR"/index.php "$DIR"/.index.php; then
                echo "ERROR: Can't copy index.php to new data dir for $h" >&2
        fi
done

# organisms to collect
declare -a ORGS=(
'Anopheles gambiae'
'Arabidopsis thaliana'
'Bacillus subtilis'
'Bos taurus'
'Caenorhabditis elegans'
'Canis familiaris'
'Danio rerio'
'Drosophila melanogaster'
'Escherichia coli'
'Equus caballus'
'Gallus gallus'
'Gibberella zeae'
'Homo sapiens'
'Hordeum vulgare'
'Mus musculus'
'Mycobacterium tuberculosis'
'Oryza sativa'
'Pan troglodytes'
'Plasmodium falciparum'
'Populus trichocarpa'
'Rattus norvegicus'
'Saccharomyces cerevisiae'
'Solanum lycopersicum'
'Sus scrofa'
'Zea mays'
)

for i in "${ORGS[@]}"
do
	echo "$i"

	for j in "${TYPES[@]}"
	do
		echo "$j"
		DIR="$DATA"/"$DATE"/"$j"
		GS=${i// /_} # replace space character with _ in species name 
		OUT="$DIR"/wikipathways-"$DATE"-"$j"-"$GS".zip
		if ! wget "${URL_BASE}?species=$i&fileType=$j&tag=$TAG" -O "$OUT"; then
			echo "ERROR: Can't get file" >&2
		fi
	done
done

# now stash index files
IN_DIR="$DATA"/"$DATE"/index
mkdir "$IN_DIR"
if ! cp "$SCRIPT_DIR"/index.php "$IN_DIR"/.index.php; then
	echo "ERROR: Can't copy index.php to new index dir" >&2
fi

#############
# Database
# DO NOT MAKE PUBLIC DUMPS OF WP DB; IT CONTAINS USER ACCOUNT INFO
#############
#BKUP_DIR='/home/wikipathways/backup'
#BKUP_FILE=wikipathways-"$DATE"*
#SQL=`find "$BKUP_DIR" -name $BKUP_FILE`
#SQL_OUT="$IN_DIR"/wikipathways-"$DATE".sql.gz
#if ! cp "$SQL" "$SQL_OUT"; then
#	echo "ERROR: Can't copy sql file" >&2
#fi 

################
# Lucene Indexer
################
## TODO: the following doesn't work
#INDEX_DIR='/home/wikipathways/wp-indexer-main/index/'
#INDEX_FILE=wikipathways-"$DATE"-index.tgz
#if ! tar -zcf "$INDEX_FILE" "$INDEX_DIR"; then
#	echo "ERROR: Can't tar gzip index" >&2
#fi
#if ! mv "$INDEX_FILE" "$IN_DIR"/.; then
#	echo "ERROR: Can't move $INDEX_FILE" >&2
#fi

########################
# now retrieve GMT files
########################

# TODO: these files are not released every day. If they haven't been released
# today, the gmt directory won't have any gmt files. Should we address this?

DATE2=`date +%Y%m%d`
GMT_URL="http://data.wikipathways.org/java-bots/gmt/$DATE2"
echo $GMT_URL
GMT_DIR="$DATA"/"$DATE"/gmt
mkdir "$GMT_DIR"
if ! cp "$SCRIPT_DIR"/index.php "$GMT_DIR"/.index.php; then
        echo "ERROR: Can't copy index.php to new gmt dir" >&2
fi

for i in "${ORGS[@]}"
do
        echo "$i"
        GS=${i// /_} # replace space character with _ in species name
        OUT="$GMT_DIR"/wikipathways-"$DATE"-gmt-"$GS".gmt
        if ! wget "${GMT_URL}/gmt_wp_$GS.gmt" -O "$OUT"; then
                echo "ERROR: Can't get file" >&2
        fi
done

find "$GMT_DIR" -size 0 -print0|xargs -0 rm

#####################
# now get RDF release
#####################
RDF_DIR="$DATA"/"$DATE"/rdf
mkdir "$RDF_DIR"
if ! cp "$SCRIPT_DIR"/index.php "$RDF_DIR"/.index.php; then
        echo "ERROR: Can't copy index.php to new rdf dir" >&2
fi

OUT="$RDF_DIR"/wikipathways-"$DATE"-rdf-gpml.zip
if ! wget --no-check-certificate https://jenkins.bigcat.unimaas.nl/job/WikiPathways%20RDF%20-%20Monthly/lastSuccessfulBuild/artifact/WP2RDF/output/wikipathways-rdf-gpml.zip -O "$OUT"; then
        echo "ERROR: Can't get file" >&2
fi

OUT="$RDF_DIR"/wikipathways-"$DATE"-rdf-wp.zip
if ! wget --no-check-certificate https://jenkins.bigcat.unimaas.nl/job/WikiPathways%20RDF%20-%20Monthly/lastSuccessfulBuild/artifact/WP2RDF/output/wikipathways-rdf-wp.zip -O "$OUT"; then
        echo "ERROR: Can't get file" >&2
fi

OUT="$RDF_DIR"/wikipathways-"$DATE"-rdf-void.ttl
if ! wget --no-check-certificate https://jenkins.bigcat.unimaas.nl/job/WikiPathways%20RDF%20-%20Monthly/lastSuccessfulBuild/artifact/WP2RDF/output/void_for_data.wp.org.ttl -O "$OUT"; then
        echo "ERROR: Can't get file" >&2
fi

###############################################################
# create symlink "./current" pointing to latest dated directory
###############################################################

# a relative symlink means the data directory will still work if copied
PREV_DIR="$(pwd)"
cd "$DATA"

CUR_DIR=current

# if it's a symlink, delete it
if [ -h "$CUR_DIR" ]; then
  rm "$CUR_DIR"

# if exists but isn't a symlink, throw an error b/c this is unexpected
elif [ -e "$CUR_DIR" ]; then
  echo "$PROGNAME: error: $(pwd)/$CUR_DIR exists but isn't a symlink" >&2
  # To unavoid anything unexpected, go back to previous directory
  cd "$PREV_DIR"
  exit 1
fi

ln -s "$DATE" "$CUR_DIR"

# To unavoid anything unexpected, go back to previous directory
cd "$PREV_DIR"
